{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/fredblair/transformers-for-stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-24 21:10:00.498152: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-24 21:10:00.500670: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-24 21:10:00.544854: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-24 21:10:00.544875: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-24 21:10:00.544897: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-02-24 21:10:00.553837: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-02-24 21:10:00.554494: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-24 21:10:01.446271: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,InputLayer, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "#from tcn import TCN,tcn_full_summary\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "#import tensorflow as tf\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "import keras_tuner\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer model\n",
    "n_timesteps, n_features = 240, 1\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim,\n",
    "                        dropout=0.5, attention_axes=None):\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "      key_dim=head_size, num_heads=num_heads, dropout=dropout,\n",
    "      attention_axes=attention_axes\n",
    "      )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res\n",
    "\n",
    "def build_transformer(head_size, \n",
    "                      num_heads,\n",
    "                      ff_dim,\n",
    "                      num_trans_blocks,\n",
    "                      mlp_units, dropout, mlp_dropout, activation) -> tf.keras.Model:\n",
    "    #n_timesteps, n_features, n_outputs = 240, 1, 1\n",
    "    inputs = tf.keras.Input(shape=(n_timesteps, n_features))\n",
    "    x = inputs \n",
    "    \n",
    "    for _ in range(num_trans_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=activation)(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    return tf.keras.Model(inputs, outputs)\n",
    "\n",
    "\n",
    "def build_model(hp):\n",
    "    head_size = hp.Choice(\"head_size\",[32,64,128,256]) #embeding size for attention\n",
    "    num_heads = hp.Choice(\"num_heads\",[4,8,16,32])  #number of attention head\n",
    "    ff_dim =  hp.Choice(\"ff_dim\",[8,16,32,64])# hidden layer size in FNN insider transformer\n",
    "    activation = hp.Choice(\"activation\",[\"elu\",\"relu\",\"selu\",\"tanh\"])\n",
    "    num_trans_blocks=  hp.Choice(\"num_trans_blocks\",[8,16,32,64])\n",
    "    #dropout = hp.Choice(\"drop_out\",[0.3,0.5,0.7])\n",
    "    #mlp_dropout= hp.Choice(\"mlp_dropout\",[0.3,0.5,0.7])\n",
    "    model = build_transformer(head_size, num_heads,ff_dim,num_trans_blocks,[256], 0.5, 0.5,activation)\n",
    "    hp_lr = hp.Choice('learning_rate', values=[1e-6,1e-7,1e-8])\n",
    "    hp_optimizer = hp.Choice('optimizer', values=['sgd', 'rmsprop', 'adam',\"adamax\"])\n",
    "    if hp_optimizer == 'sgd':\n",
    "        optimizer = keras.optimizers.SGD(learning_rate=hp_lr)\n",
    "    elif hp_optimizer == 'rmsprop':\n",
    "        optimizer = keras.optimizers.RMSprop(learning_rate=hp_lr)\n",
    "    elif hp_optimizer == 'adam':\n",
    "        optimizer = keras.optimizers.Adam(learning_rate=hp_lr)\n",
    "    elif hp_optimizer == 'adamax':\n",
    "        optimizer = keras.optimizers.Adamax(learning_rate=hp_lr)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer choice\")\n",
    "\n",
    "    model.compile(optimizer = hp_optimizer,loss=keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 70 Complete [05h 53m 03s]\n",
      "val_accuracy: 0.5526494383811951\n",
      "\n",
      "Best val_accuracy So Far: 0.55910325050354\n",
      "Total elapsed time: 19d 18h 01m 08s\n",
      "{'head_size': 64, 'num_heads': 16, 'ff_dim': 16, 'activation': 'relu', 'num_trans_blocks': 16, 'learning_rate': 1e-06, 'optimizer': 'adam'}\n",
      "Epoch 1/1000\n",
      "368/368 [==============================] - 1328s 4s/step - loss: 0.8230 - accuracy: 0.5055 - val_loss: 0.7103 - val_accuracy: 0.5126\n",
      "Epoch 2/1000\n",
      "368/368 [==============================] - 1277s 3s/step - loss: 0.7042 - accuracy: 0.5436 - val_loss: 0.7010 - val_accuracy: 0.5099\n",
      "Epoch 3/1000\n",
      "368/368 [==============================] - 1291s 4s/step - loss: 0.6755 - accuracy: 0.5728 - val_loss: 0.7020 - val_accuracy: 0.5068\n",
      "Epoch 4/1000\n",
      "368/368 [==============================] - 1279s 3s/step - loss: 0.6567 - accuracy: 0.6036 - val_loss: 0.7008 - val_accuracy: 0.5027\n",
      "Epoch 5/1000\n",
      "368/368 [==============================] - 1286s 3s/step - loss: 0.6410 - accuracy: 0.6303 - val_loss: 0.7037 - val_accuracy: 0.5221\n",
      "Epoch 6/1000\n",
      "368/368 [==============================] - 1284s 3s/step - loss: 0.6199 - accuracy: 0.6570 - val_loss: 0.7092 - val_accuracy: 0.5251\n",
      "Epoch 7/1000\n",
      "368/368 [==============================] - 1277s 3s/step - loss: 0.6000 - accuracy: 0.6815 - val_loss: 0.7149 - val_accuracy: 0.5299\n",
      "Epoch 8/1000\n",
      "368/368 [==============================] - 1292s 4s/step - loss: 0.5790 - accuracy: 0.6979 - val_loss: 0.7335 - val_accuracy: 0.5292\n",
      "Epoch 9/1000\n",
      "368/368 [==============================] - 1276s 3s/step - loss: 0.5542 - accuracy: 0.7228 - val_loss: 0.7386 - val_accuracy: 0.5343\n",
      "Epoch 10/1000\n",
      "368/368 [==============================] - 1281s 3s/step - loss: 0.5356 - accuracy: 0.7377 - val_loss: 0.7461 - val_accuracy: 0.5329\n",
      "Epoch 11/1000\n",
      "368/368 [==============================] - 1259s 3s/step - loss: 0.5176 - accuracy: 0.7572 - val_loss: 0.7419 - val_accuracy: 0.5394\n",
      "Epoch 12/1000\n",
      "368/368 [==============================] - 1279s 3s/step - loss: 0.4956 - accuracy: 0.7745 - val_loss: 0.7861 - val_accuracy: 0.5516\n",
      "Epoch 13/1000\n",
      "368/368 [==============================] - 1318s 4s/step - loss: 0.4836 - accuracy: 0.7807 - val_loss: 0.7757 - val_accuracy: 0.5326\n",
      "Epoch 14/1000\n",
      "368/368 [==============================] - 1299s 4s/step - loss: 0.4652 - accuracy: 0.7924 - val_loss: 0.8266 - val_accuracy: 0.5343\n",
      "Model saved to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "Training end: 2024-02-29 13:33:48\n",
      "224/224 [==============================] - 230s 1s/step\n",
      "Overall Accuracy for test set:0.4986711428171772\n",
      "Prediction for period 0 successfully saved.\n",
      "(14569, 240, 1)\n",
      "(14569, 1)\n",
      "(7500, 240, 1)\n",
      "(7500, 1)\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Training the model for Training Set 1 from 2024-02-29 13:37:43\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Model restore from /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "Epoch 1/1000\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.6220 - accuracy: 0.6420\n",
      "Epoch 1: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "365/365 [==============================] - 1260s 3s/step - loss: 0.6220 - accuracy: 0.6420 - val_loss: 0.7621 - val_accuracy: 0.5154\n",
      "Epoch 2/1000\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.5717 - accuracy: 0.6782\n",
      "Epoch 2: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "365/365 [==============================] - 1274s 3s/step - loss: 0.5717 - accuracy: 0.6782 - val_loss: 0.7938 - val_accuracy: 0.5086\n",
      "Epoch 3/1000\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.5528 - accuracy: 0.7003\n",
      "Epoch 3: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "365/365 [==============================] - 1277s 3s/step - loss: 0.5528 - accuracy: 0.7003 - val_loss: 0.7365 - val_accuracy: 0.5202\n",
      "Epoch 4/1000\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.5259 - accuracy: 0.7242\n",
      "Epoch 4: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "365/365 [==============================] - 1257s 3s/step - loss: 0.5259 - accuracy: 0.7242 - val_loss: 0.7787 - val_accuracy: 0.5117\n",
      "Epoch 5/1000\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.5080 - accuracy: 0.7396\n",
      "Epoch 5: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "365/365 [==============================] - 1237s 3s/step - loss: 0.5080 - accuracy: 0.7396 - val_loss: 0.8255 - val_accuracy: 0.5124\n",
      "Epoch 6/1000\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.4890 - accuracy: 0.7586\n",
      "Epoch 6: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "365/365 [==============================] - 1219s 3s/step - loss: 0.4890 - accuracy: 0.7586 - val_loss: 0.8076 - val_accuracy: 0.5202\n",
      "Epoch 7/1000\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.4727 - accuracy: 0.7709\n",
      "Epoch 7: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "365/365 [==============================] - 1240s 3s/step - loss: 0.4727 - accuracy: 0.7709 - val_loss: 0.8209 - val_accuracy: 0.5305\n",
      "Epoch 8/1000\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.4612 - accuracy: 0.7850\n",
      "Epoch 8: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "365/365 [==============================] - 1232s 3s/step - loss: 0.4612 - accuracy: 0.7850 - val_loss: 0.7971 - val_accuracy: 0.5261\n",
      "Epoch 9/1000\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.4514 - accuracy: 0.7916\n",
      "Epoch 9: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "365/365 [==============================] - 1242s 3s/step - loss: 0.4514 - accuracy: 0.7916 - val_loss: 0.8584 - val_accuracy: 0.5281\n",
      "Epoch 10/1000\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.4368 - accuracy: 0.7976\n",
      "Epoch 10: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "365/365 [==============================] - 1210s 3s/step - loss: 0.4368 - accuracy: 0.7976 - val_loss: 0.8733 - val_accuracy: 0.5295\n",
      "Epoch 11/1000\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.4265 - accuracy: 0.8063\n",
      "Epoch 11: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "365/365 [==============================] - 1252s 3s/step - loss: 0.4265 - accuracy: 0.8063 - val_loss: 0.8378 - val_accuracy: 0.5381\n",
      "Epoch 12/1000\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.4218 - accuracy: 0.8160\n",
      "Epoch 12: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "365/365 [==============================] - 1262s 3s/step - loss: 0.4218 - accuracy: 0.8160 - val_loss: 0.9317 - val_accuracy: 0.5240\n",
      "Epoch 13/1000\n",
      "365/365 [==============================] - ETA: 0s - loss: 0.4067 - accuracy: 0.8208\n",
      "Epoch 13: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "365/365 [==============================] - 1268s 3s/step - loss: 0.4067 - accuracy: 0.8208 - val_loss: 1.0023 - val_accuracy: 0.5237\n",
      "Model saved to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "Training end: 2024-02-29 18:08:14\n",
      "235/235 [==============================] - 230s 980ms/step\n",
      "Overall Accuracy for test set:0.4981333333333333\n",
      "Prediction for period 1 successfully saved.\n",
      "(14919, 240, 1)\n",
      "(14919, 1)\n",
      "(7448, 240, 1)\n",
      "(7448, 1)\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Training the model for Training Set 2 from 2024-02-29 18:12:08\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Model restore from /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "373/373 [==============================] - ETA: 0s - loss: 0.6477 - accuracy: 0.6248\n",
      "Epoch 1: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "373/373 [==============================] - 1272s 3s/step - loss: 0.6477 - accuracy: 0.6248 - val_loss: 0.7625 - val_accuracy: 0.5147\n",
      "Epoch 2/1000\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.6160 - accuracy: 0.6541\n",
      "Epoch 2: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "373/373 [==============================] - 1268s 3s/step - loss: 0.6160 - accuracy: 0.6541 - val_loss: 0.7990 - val_accuracy: 0.5013\n",
      "Epoch 3/1000\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.5718 - accuracy: 0.6823\n",
      "Epoch 3: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "373/373 [==============================] - 1310s 4s/step - loss: 0.5718 - accuracy: 0.6823 - val_loss: 0.7754 - val_accuracy: 0.5060\n",
      "Epoch 4/1000\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.5446 - accuracy: 0.7087\n",
      "Epoch 4: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "373/373 [==============================] - 1271s 3s/step - loss: 0.5446 - accuracy: 0.7087 - val_loss: 0.8046 - val_accuracy: 0.5094\n",
      "Epoch 5/1000\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.5224 - accuracy: 0.7245\n",
      "Epoch 5: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "373/373 [==============================] - 1302s 3s/step - loss: 0.5224 - accuracy: 0.7245 - val_loss: 0.8590 - val_accuracy: 0.5161\n",
      "Epoch 6/1000\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.4989 - accuracy: 0.7436\n",
      "Epoch 6: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "373/373 [==============================] - 1276s 3s/step - loss: 0.4989 - accuracy: 0.7436 - val_loss: 0.8615 - val_accuracy: 0.5168\n",
      "Epoch 7/1000\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.4752 - accuracy: 0.7623\n",
      "Epoch 7: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "373/373 [==============================] - 1309s 4s/step - loss: 0.4752 - accuracy: 0.7623 - val_loss: 0.8995 - val_accuracy: 0.5214\n",
      "Epoch 8/1000\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.4532 - accuracy: 0.7809\n",
      "Epoch 8: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "373/373 [==============================] - 1294s 3s/step - loss: 0.4532 - accuracy: 0.7809 - val_loss: 0.9830 - val_accuracy: 0.5158\n",
      "Epoch 9/1000\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.4345 - accuracy: 0.7949\n",
      "Epoch 9: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "373/373 [==============================] - 1312s 4s/step - loss: 0.4345 - accuracy: 0.7949 - val_loss: 0.9030 - val_accuracy: 0.5214\n",
      "Epoch 10/1000\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.4189 - accuracy: 0.8094\n",
      "Epoch 10: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "373/373 [==============================] - 1299s 3s/step - loss: 0.4189 - accuracy: 0.8094 - val_loss: 1.0235 - val_accuracy: 0.5225\n",
      "Epoch 11/1000\n",
      "373/373 [==============================] - ETA: 0s - loss: 0.3976 - accuracy: 0.8188\n",
      "Epoch 11: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "373/373 [==============================] - 1313s 4s/step - loss: 0.3976 - accuracy: 0.8188 - val_loss: 1.0202 - val_accuracy: 0.5171\n",
      "Model saved to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "Training end: 2024-02-29 22:09:12\n",
      "233/233 [==============================] - 236s 1s/step\n",
      "Overall Accuracy for test set:0.5112781954887218\n",
      "Prediction for period 2 successfully saved.\n",
      "(15248, 240, 1)\n",
      "(15248, 1)\n",
      "(7058, 240, 1)\n",
      "(7058, 1)\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Training the model for Training Set 3 from 2024-02-29 22:13:12\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Model restore from /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "Epoch 1/1000\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.6954 - accuracy: 0.5152\n",
      "Epoch 1: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "382/382 [==============================] - 1304s 3s/step - loss: 0.6954 - accuracy: 0.5152 - val_loss: 0.6949 - val_accuracy: 0.4882\n",
      "Epoch 2/1000\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.6927 - accuracy: 0.5174\n",
      "Epoch 2: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "382/382 [==============================] - 1287s 3s/step - loss: 0.6927 - accuracy: 0.5174 - val_loss: 0.7441 - val_accuracy: 0.5161\n",
      "Epoch 3/1000\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.6867 - accuracy: 0.5485\n",
      "Epoch 3: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "382/382 [==============================] - 1318s 3s/step - loss: 0.6867 - accuracy: 0.5485 - val_loss: 0.7640 - val_accuracy: 0.5066\n",
      "Epoch 4/1000\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.6820 - accuracy: 0.5648\n",
      "Epoch 4: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "382/382 [==============================] - 1322s 3s/step - loss: 0.6820 - accuracy: 0.5648 - val_loss: 0.7546 - val_accuracy: 0.5184\n",
      "Epoch 5/1000\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.6849 - accuracy: 0.5290\n",
      "Epoch 5: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "382/382 [==============================] - 1342s 4s/step - loss: 0.6849 - accuracy: 0.5290 - val_loss: 0.7078 - val_accuracy: 0.4931\n",
      "Epoch 6/1000\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.6751 - accuracy: 0.5656\n",
      "Epoch 6: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "382/382 [==============================] - 1320s 3s/step - loss: 0.6751 - accuracy: 0.5656 - val_loss: 0.7832 - val_accuracy: 0.5111\n",
      "Epoch 7/1000\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.6548 - accuracy: 0.5966\n",
      "Epoch 7: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "382/382 [==============================] - 1340s 4s/step - loss: 0.6548 - accuracy: 0.5966 - val_loss: 0.7397 - val_accuracy: 0.5082\n",
      "Epoch 8/1000\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.6273 - accuracy: 0.6316\n",
      "Epoch 8: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "382/382 [==============================] - 1306s 3s/step - loss: 0.6273 - accuracy: 0.6316 - val_loss: 0.7958 - val_accuracy: 0.5079\n",
      "Epoch 9/1000\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.6081 - accuracy: 0.6504\n",
      "Epoch 9: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "382/382 [==============================] - 1329s 3s/step - loss: 0.6081 - accuracy: 0.6504 - val_loss: 0.7848 - val_accuracy: 0.5213\n",
      "Epoch 10/1000\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.5733 - accuracy: 0.6828\n",
      "Epoch 10: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "382/382 [==============================] - 1282s 3s/step - loss: 0.5733 - accuracy: 0.6828 - val_loss: 0.8887 - val_accuracy: 0.5039\n",
      "Epoch 11/1000\n",
      "382/382 [==============================] - ETA: 0s - loss: 0.5475 - accuracy: 0.7045\n",
      "Epoch 11: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "382/382 [==============================] - 1299s 3s/step - loss: 0.5475 - accuracy: 0.7045 - val_loss: 0.8971 - val_accuracy: 0.5108\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "Training end: 2024-03-01 02:13:59\n",
      "221/221 [==============================] - 215s 975ms/step\n",
      "Overall Accuracy for test set:0.5069424766222727\n",
      "Prediction for period 3 successfully saved.\n",
      "(14806, 240, 1)\n",
      "(14806, 1)\n",
      "(7175, 240, 1)\n",
      "(7175, 1)\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Training the model for Training Set 4 from 2024-03-01 02:17:40\n",
      "-------------------------------------------------------------------------------------------------------\n",
      "Model restore from /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "Epoch 1/1000\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.6992 - accuracy: 0.5077\n",
      "Epoch 1: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "371/371 [==============================] - 1218s 3s/step - loss: 0.6992 - accuracy: 0.5077 - val_loss: 0.6959 - val_accuracy: 0.4831\n",
      "Epoch 2/1000\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.6932 - accuracy: 0.5078\n",
      "Epoch 2: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "371/371 [==============================] - 1216s 3s/step - loss: 0.6932 - accuracy: 0.5078 - val_loss: 0.6990 - val_accuracy: 0.4821\n",
      "Epoch 3/1000\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.6933 - accuracy: 0.5074\n",
      "Epoch 3: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "371/371 [==============================] - 1221s 3s/step - loss: 0.6933 - accuracy: 0.5074 - val_loss: 0.6977 - val_accuracy: 0.4845\n",
      "Epoch 4/1000\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.6912 - accuracy: 0.5101\n",
      "Epoch 4: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "371/371 [==============================] - 1217s 3s/step - loss: 0.6912 - accuracy: 0.5101 - val_loss: 0.6985 - val_accuracy: 0.4892\n",
      "Epoch 5/1000\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.6915 - accuracy: 0.5122\n",
      "Epoch 5: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "371/371 [==============================] - 1217s 3s/step - loss: 0.6915 - accuracy: 0.5122 - val_loss: 0.7084 - val_accuracy: 0.4929\n",
      "Epoch 6/1000\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.6890 - accuracy: 0.5160\n",
      "Epoch 6: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "371/371 [==============================] - 1221s 3s/step - loss: 0.6890 - accuracy: 0.5160 - val_loss: 0.7145 - val_accuracy: 0.4980\n",
      "Epoch 7/1000\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.6874 - accuracy: 0.5303\n",
      "Epoch 7: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "371/371 [==============================] - 1277s 3s/step - loss: 0.6874 - accuracy: 0.5303 - val_loss: 0.7303 - val_accuracy: 0.4993\n",
      "Epoch 8/1000\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.6866 - accuracy: 0.5352\n",
      "Epoch 8: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "371/371 [==============================] - 1224s 3s/step - loss: 0.6866 - accuracy: 0.5352 - val_loss: 0.7424 - val_accuracy: 0.4997\n",
      "Epoch 9/1000\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.6835 - accuracy: 0.5328\n",
      "Epoch 9: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "371/371 [==============================] - 1220s 3s/step - loss: 0.6835 - accuracy: 0.5328 - val_loss: 0.7373 - val_accuracy: 0.5064\n",
      "Epoch 10/1000\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.6851 - accuracy: 0.5366\n",
      "Epoch 10: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "371/371 [==============================] - 1154s 3s/step - loss: 0.6851 - accuracy: 0.5366 - val_loss: 0.6998 - val_accuracy: 0.4862\n",
      "Epoch 11/1000\n",
      "371/371 [==============================] - ETA: 0s - loss: 0.6900 - accuracy: 0.5165\n",
      "Epoch 11: saving model to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "371/371 [==============================] - 1152s 3s/step - loss: 0.6900 - accuracy: 0.5165 - val_loss: 0.6973 - val_accuracy: 0.4949\n",
      "Model saved to /home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5\n",
      "Training end: 2024-03-01 05:59:59\n",
      "225/225 [==============================] - 201s 893ms/step\n",
      "Overall Accuracy for test set:0.49881533101045294\n",
      "Prediction for period 4 successfully saved.\n"
     ]
    }
   ],
   "source": [
    "timesteps = 240\n",
    "num_input =1\n",
    "num_classes=1\n",
    "label = list(range(timesteps)) + ['target'] + ['ticker'] + ['target_date'] + ['sector']\n",
    "\n",
    "training_data = []\n",
    "training_label = []\n",
    "testing_data =[]\n",
    "testing_label =[]\n",
    "\n",
    "accuracy_results = []\n",
    "\n",
    "for i in range(5):\n",
    "    # read the data\n",
    "    path = '/home/RDC/yeungwin/H:/yeungwin/DAX/data/'\n",
    "    train = pd.read_csv(path+'Set_' + str(i) + '_Train.csv', index_col=0).dropna()\n",
    "    test = pd.read_csv(path+'Set_' + str(i) + '_Test.csv', index_col=0).dropna()\n",
    "\n",
    "    train.columns = label\n",
    "    test.columns = label\n",
    "\n",
    "    train_label = train.iloc[:, timesteps]\n",
    "    train_data = train.iloc[:, :timesteps]\n",
    "    test_label = test.iloc[:,timesteps]\n",
    "    test_data = test.iloc[:, :timesteps]\n",
    "    \n",
    "    \n",
    "     # reshape input\n",
    "    #  data: (samples, timesteps, features)\n",
    "    x_train = np.array(train_data).reshape((len(train_data), timesteps, num_input), order = 'F')\n",
    "    x_test = np.array(test_data).reshape((len(test_data), timesteps, num_input), order = 'F')\n",
    "    # label: (samples, target)\n",
    "    y_train = np.array(train_label).reshape((len(train_label), num_classes))\n",
    "    y_test = np.array(test_label).reshape((len(test_label), num_classes))\n",
    "        \n",
    "    print(x_train.shape)\n",
    "    print(y_train.shape)\n",
    "    print(x_test.shape)\n",
    "    print(y_test.shape)\n",
    "    \n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "    print(\"Training the model for Training Set \" + str(i) + \" from \" +\n",
    "    datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d %H:%M:%S'))\n",
    "    print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "        \n",
    "    if i==0:\n",
    "        tuner = keras_tuner.BayesianOptimization(build_model,\n",
    "            objective='val_accuracy', #overwrite=True,\n",
    "            max_trials=70, directory='tf3', seed=999)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience =10)\n",
    "        tuner.search(x_train,y_train, epochs =1000,validation_split=0.2, callbacks=[early_stop])\n",
    "\n",
    "        # save the best model\n",
    "        #hypermodel =build_model\n",
    "        best_hp = tuner.get_best_hyperparameters()[0]\n",
    "        best_model = build_model(best_hp)\n",
    "        print(tuner.get_best_hyperparameters()[0].get_config()[\"values\"])\n",
    "\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10,restore_best_weights=False)\n",
    "        result = best_model.fit(x_train,y_train, epochs=1000, validation_split =0.2, verbose =1, callbacks=[early_stop])\n",
    "\n",
    "        \n",
    "    else:\n",
    "        load_path = '/home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5'\n",
    "        print('Model restore from ' + load_path)\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=load_path,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "        early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                    patience = 10,restore_best_weights=False)\n",
    "\n",
    "        result = best_model.fit(\n",
    "            x_train, \n",
    "            y_train, \n",
    "            epochs = 1000, \n",
    "            validation_split=0.2,\n",
    "            verbose =1,\n",
    "            callbacks=[cp_callback, early_stop]        \n",
    "        ) \n",
    "        \n",
    "    save_path =  '/home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight3.h5'\n",
    "    best_model.save_weights(save_path)\n",
    "    print(\"Model saved to \" + save_path)\n",
    "    print(\"Training end: \" + datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d %H:%M:%S'))\n",
    "        \n",
    "    ##make prediction\n",
    "    pred_ff_test = best_model.predict(x_test)\n",
    "    #pred = pred_ff_test.tolist()\n",
    "    pred = pred_ff_test.reshape((1, len(pred_ff_test))).tolist()[0]\n",
    "    output_data = pd.DataFrame({'y_prob': pred, 'y_true': test['target'], 'Ticker': test['ticker'],\n",
    "                                    'Date': test['target_date'], 'Sector': test['sector'], })\n",
    "    accuracy = accuracy_score(np.round(output_data['y_prob']), output_data['y_true'])\n",
    "    print('Overall Accuracy for test set:'+ str(accuracy))\n",
    "    \n",
    "    \n",
    "    output_data.to_csv('/home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/pred3/tf_prediction_period_'+str(i)+'.csv')\n",
    "    print('Prediction for period ' + str(i) + ' successfully saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for each sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector = pd.read_csv('/home/RDC/yeungwin/H:/yeungwin/DAX/8_performance/stock_analysis.csv')\n",
    "sector_list = sector[\"Sector\"].unique().tolist()\n",
    "print(sector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 Complete [00h 00m 08s]\n",
      "\n",
      "Best val_accuracy So Far: None\n",
      "Total elapsed time: 00h 00m 18s\n",
      "\n",
      "Search: Running Trial #3\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "128               |128               |head_size\n",
      "8                 |8                 |num_heads\n",
      "2                 |2                 |ff_dim\n",
      "tanh              |elu               |activation\n",
      "8                 |16                |num_trans_blocks\n",
      "1e-07             |1e-08             |learning_rate\n",
      "rmsprop           |sgd               |optimizer\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py\", line 273, in _try_run_and_update_trial\n",
      "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
      "  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py\", line 238, in _run_and_update_trial\n",
      "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n",
      "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n",
      "    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n",
      "    return model.fit(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n",
      "    raise e.with_traceback(filtered_tb) from None\n",
      "  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras/src/engine/data_adapter.py\", line 1795, in train_validation_split\n",
      "    raise ValueError(\n",
      "ValueError: Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py\", line 273, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py\", line 238, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras/src/engine/data_adapter.py\", line 1795, in train_validation_split\n    raise ValueError(\nValueError: Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 54\u001b[0m\n\u001b[1;32m     50\u001b[0m tuner \u001b[38;5;241m=\u001b[39m keras_tuner\u001b[38;5;241m.\u001b[39mBayesianOptimization(build_model,\n\u001b[1;32m     51\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;66;03m# overwrite=True,\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     max_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39msector, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m111\u001b[39m)\n\u001b[1;32m     53\u001b[0m early_stop \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m )\n\u001b[0;32m---> 54\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(x_train,y_train, epochs \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stop])\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# save the best model\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#hypermodel =build_model\u001b[39;00m\n\u001b[1;32m     58\u001b[0m best_hp \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:338\u001b[0m, in \u001b[0;36mBaseTuner.on_trial_end\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_trial_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Called at the end of a trial.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m        trial: A `Trial` instance.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mend_trial(trial)\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/oracle.py:108\u001b[0m, in \u001b[0;36msynchronized.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m     LOCKS[oracle]\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m    107\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m thread_name\n\u001b[0;32m--> 108\u001b[0m ret_val \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m need_acquire:\n\u001b[1;32m    110\u001b[0m     THREADS[oracle] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/oracle.py:577\u001b[0m, in \u001b[0;36mOracle.end_trial\u001b[0;34m(self, trial)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retry(trial):\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_order\u001b[38;5;241m.\u001b[39mappend(trial\u001b[38;5;241m.\u001b[39mtrial_id)\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_consecutive_failures()\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_trial(trial)\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/oracle.py:534\u001b[0m, in \u001b[0;36mOracle._check_consecutive_failures\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    532\u001b[0m     consecutive_failures \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consecutive_failures \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials:\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    535\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of consecutive failures exceeded the limit \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    536\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_consecutive_failed_trials\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;241m+\u001b[39m (trial\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    538\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Number of consecutive failures exceeded the limit of 3.\nTraceback (most recent call last):\n  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py\", line 273, in _try_run_and_update_trial\n    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py\", line 238, in _run_and_update_trial\n    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 314, in run_trial\n    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py\", line 233, in _build_and_fit_model\n    results = self.hypermodel.fit(hp, model, *args, **kwargs)\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras_tuner/src/engine/hypermodel.py\", line 149, in fit\n    return model.fit(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/home/RDC/yeungwin/.conda/envs/tf/lib/python3.11/site-packages/keras/src/engine/data_adapter.py\", line 1795, in train_validation_split\n    raise ValueError(\nValueError: Training data contains 0 samples, which is not sufficient to split it into a validation and training set as specified by `validation_split=0.2`. Either provide more data, or a different value for the `validation_split` argument.\n"
     ]
    }
   ],
   "source": [
    "timesteps = 240\n",
    "num_input =1\n",
    "num_classes=1\n",
    "label = list(range(timesteps)) + ['target'] + ['ticker'] + ['target_date'] + ['sector']\n",
    "\n",
    "training_data = []\n",
    "training_label = []\n",
    "testing_data =[]\n",
    "testing_label =[]\n",
    "\n",
    "accuracy_results = []\n",
    "\n",
    "for sector in sector_list:\n",
    "    for i in range(5):\n",
    "        # read the data\n",
    "        path = '/home/RDC/yeungwin/H:/yeungwin/DAX/data/'\n",
    "        train = pd.read_csv(path+'Set_' + str(i) + '_Train.csv', index_col=0).dropna()\n",
    "        train = train[train.sector == sector]\n",
    "        test = pd.read_csv(path+'Set_' + str(i) + '_Test.csv', index_col=0).dropna()\n",
    "        test = test[test.sector == sector]\n",
    "\n",
    "        train.columns = label\n",
    "        test.columns = label\n",
    "\n",
    "        train_label = train.iloc[:, timesteps]\n",
    "        train_data = train.iloc[:, :timesteps]\n",
    "        test_label = test.iloc[:,timesteps]\n",
    "        test_data = test.iloc[:, :timesteps]\n",
    "\n",
    "\n",
    "         # reshape input\n",
    "        #  data: (samples, timesteps, features)\n",
    "        x_train = np.array(train_data).reshape((len(train_data), timesteps, num_input), order = 'F')\n",
    "        x_test = np.array(test_data).reshape((len(test_data), timesteps, num_input), order = 'F')\n",
    "        # label: (samples, target)\n",
    "        y_train = np.array(train_label).reshape((len(train_label), num_classes))\n",
    "        y_test = np.array(test_label).reshape((len(test_label), num_classes))\n",
    "\n",
    "        print(x_train.shape)\n",
    "        print(y_train.shape)\n",
    "        print(x_test.shape)\n",
    "        print(y_test.shape)\n",
    "\n",
    "        print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Training the model for Training Set \" + str(i) + \" from \" +\n",
    "        datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d %H:%M:%S'))\n",
    "        print(\"-------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "        if i==0:\n",
    "            tuner = keras_tuner.BayesianOptimization(build_model,\n",
    "                objective='val_accuracy',# overwrite=True,\n",
    "                max_trials=10, directory='tf_'+sector, seed=111)\n",
    "            early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience =3 )\n",
    "            tuner.search(x_train,y_train, epochs =100,validation_split=0.2, callbacks=[early_stop])\n",
    "\n",
    "            # save the best model\n",
    "            #hypermodel =build_model\n",
    "            best_hp = tuner.get_best_hyperparameters()[0]\n",
    "            best_model = build_model(best_hp)\n",
    "            print(tuner.get_best_hyperparameters()[0].get_config()[\"values\"])\n",
    "\n",
    "            early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 10)\n",
    "            result = best_model.fit(x_train,y_train, epochs=1000, batch_size=64,validation_split =0.2, verbose =1, callbacks=[early_stop])\n",
    "\n",
    "        else:\n",
    "            load_path = '/home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight_'+sector+'.h5'\n",
    "            print('Model restore from ' + load_path)\n",
    "            cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=load_path,\n",
    "                                                     save_weights_only=True,\n",
    "                                                     verbose=1)\n",
    "            early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                        patience = 10)\n",
    "\n",
    "            result = best_model.fit(\n",
    "                x_train, \n",
    "                y_train, \n",
    "                epochs = 1000, \n",
    "                validation_split=0.2,\n",
    "                verbose =1,\n",
    "                callbacks=[cp_callback, early_stop]        \n",
    "            ) \n",
    "            \n",
    "\n",
    "        save_path = \"/home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/transformer_model_weight/tf_weight_\"+sector+\".h5\"\n",
    "        best_model.save_weights(save_path)\n",
    "        print(\"Model saved to \" + save_path)\n",
    "        print(\"Training end: \" + datetime.datetime.strftime(datetime.datetime.now(), '%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "        ##make prediction\n",
    "        pred_ff_test = best_model.predict(x_test)\n",
    "        #pred = pred_ff_test.tolist()\n",
    "        pred = pred_ff_test.reshape((1, len(pred_ff_test))).tolist()[0]\n",
    "        output_data = pd.DataFrame({'y_prob': pred, 'y_true': test['target'], 'Ticker': test['ticker'],\n",
    "                                        'Date': test['target_date'], 'Sector': test['sector'], })\n",
    "        accuracy = accuracy_score(np.round(output_data['y_prob']), output_data['y_true'])\n",
    "        print('Overall Accuracy for test set:'+ str(accuracy))\n",
    "\n",
    "\n",
    "        output_data.to_csv('/home/RDC/yeungwin/H:/yeungwin/DAX/6_Transformer/tf_pred/tf_prediction_period_'+sector+'_'+str(i)+'.csv')\n",
    "        print('Prediction for period ' + str(i) + ' successfully saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-tf]",
   "language": "python",
   "name": "conda-env-.conda-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
